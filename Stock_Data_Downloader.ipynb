{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed717dd",
   "metadata": {},
   "source": [
    "# üìä Stock Intraday Data Downloader\n",
    "\n",
    "**Automated system to download 30-minute interval data from Yahoo Finance**\n",
    "\n",
    "- **Data Sources**: Custom ticker list from `tickers.csv`\n",
    "- **Interval**: 30 minutes\n",
    "- **Storage**: Compressed CSV files (`.csv.gz`)\n",
    "- **Mode**: Incremental append (no overwrites)\n",
    "- **Logging**: Full error tracking in `download_log.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d353ca",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Installation & Setup\n",
    "\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfb1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d773782",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e29272b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration complete\n",
      "üìÅ Data directory: c:\\Users\\trion\\OneDrive\\Desktop\\Files\\Stock Prices Dataset\\stock_data\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress yfinance warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', module='yfinance')\n",
    "\n",
    "# Configure logging (only to file, not console)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('download_log.txt')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create data directory\n",
    "DATA_DIR = Path(\"stock_data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration complete\")\n",
    "print(f\"üìÅ Data directory: {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec014c",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Ticker List from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148f6ed",
   "metadata": {},
   "source": [
    "Russell 1000 Companies as of August 22, 2025, are 1007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849fdff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1007 tickers (embedded)\n"
     ]
    }
   ],
   "source": [
    "# Embedded ticker list (replaces reading tickers.csv).\n",
    "# The full ticker list is written here so the notebook won't call tickers.csv at runtime.\n",
    "all_tickers = [\"MMM\",\"AOS\",\"AAON\",\"ABT\",\"ABBV\",\"ACHC\",\"ACN\",\"AYI\",\"ADBE\",\"ADT\",\"WMS\",\"AMD\",\n",
    "               \"ACM\",\"AES\",\"AMG\",\"AFRM\",\"AFL\",\"AGCO\",\"A\",\"ADC\",\"AGNC\",\"AL\",\"APD\",\"ABNB\",\"AKAM\",\n",
    "               \"ALK\",\"ALB\",\"ACI\",\"AA\",\"ARE\",\"ALGN\",\"ALLE\",\"ALGM\",\"LNT\",\"ALSN\",\"ALL\",\"ALLY\",\n",
    "               \"ALNY\",\"GOOGL\",\"GOOG\",\"MO\",\"AMZN\",\"AMCR\",\"DOX\",\"AMTM\",\"AS\",\"AEE\",\"AAL\",\"AEP\",\n",
    "               \"AXP\",\"AFG\",\"AMH\",\"AIG\",\"AMT\",\"AWK\",\"COLD\",\"AMP\",\"AME\",\"AMGN\",\"AMKR\",\"APH\",\"ADI\",\n",
    "               \"AU\",\"NLY\",\"AM\",\"AR\",\"AON\",\"APA\",\"APG\",\"APLS\",\"APO\",\"APPF\",\"AAPL\",\"AIT\",\"AMAT\",\n",
    "               \"APP\",\"ATR\",\"APTV\",\"ARMK\",\"ACGL\",\"ADM\",\"ARES\",\"ANET\",\"AWI\",\"ARW\",\"AJG\",\"ASH\",\n",
    "               \"AIZ\",\"AGO\",\"ALAB\",\"ASTS\",\"T\",\"ATI\",\"TEAM\",\"ATO\",\"AUR\",\"ADSK\",\"ADP\",\"AN\",\"AZO\",\n",
    "               \"AVB\",\"AVTR\",\"AVY\",\"CAR\",\"AVT\",\"AXTA\",\"AXS\",\"AXON\",\"BKR\",\"BALL\",\"BAC\",\"OZK\",\n",
    "               \"BBWI\",\"BAX\",\"BDX\",\"BRBR\",\"BSY\",\"BRK-B\",\"BBY\",\"BILL\",\"BIO\",\"TECH\",\"BIIB\",\"BMRN\",\n",
    "               \"BIRK\",\"BJ\",\"BLK\",\"BX\",\"HRB\",\"XYZ\",\"OWL\",\"BK\",\"BA\",\"BOKF\",\"BKNG\",\"BAH\",\"BWA\",\n",
    "               \"SAM\",\"BSX\",\"BYD\",\"BFAM\",\"BHF\",\"BMY\",\"BRX\",\"AVGO\",\"BR\",\"BAM\",\"BEPC\",\"BRO\",\"BF-A\",\n",
    "               \"BF-B\",\"BRKR\",\"BC\",\"BLDR\",\"BG\",\"BURL\",\"BWXT\",\"BXP\",\"CHRW\",\"CACI\",\"CDNS\",\"CZR\",\n",
    "               \"CPT\",\"CPB\",\"COF\",\"CAH\",\"CAI\",\"CSL\",\"CG\",\"KMX\",\"CCL\",\"CRS\",\"CARR\",\"CVNA\",\"CAT\",\n",
    "               \"CAVA\",\"CBOE\",\"CBRE\",\"CDW\",\"CE\",\"CELH\",\"COR\",\"CNC\",\"CNP\",\"CERT\",\"CF\",\"CRL\",\"SCHW\",\n",
    "               \"CHTR\",\"CHE\",\"LNG\",\"CVX\",\"CHWY\",\"CMG\",\"CHH\",\"CHRD\",\"CB\",\"CHD\",\"CHDN\",\"CIEN\",\"CI\",\n",
    "               \"CINF\",\"CTAS\",\"CRUS\",\"CSCO\",\"C\",\"CFG\",\"CIVI\",\"CLVT\",\"CLH\",\"CWEN-A\",\"CWEN\",\"CLF\",\n",
    "               \"CLX\",\"NET\",\"CME\",\"CMS\",\"CNA\",\"CNH\",\"KO\",\"COKE\",\"CGNX\",\"CTSH\",\"COHR\",\"COIN\",\"CL\",\n",
    "               \"COLB\",\"COLM\",\"CMCSA\",\"CMA\",\"FIX\",\"CBSH\",\"CAG\",\"CNXC\",\"CFLT\",\"COP\",\"ED\",\"STZ\",\n",
    "               \"CEG\",\"COO\",\"CPRT\",\"CORT\",\"CNM\",\"GLW\",\"CPAY\",\"CTVA\",\"CSGP\",\"COST\",\"CTRA\",\"COTY\",\n",
    "               \"CPNG\",\"CUZ\",\"CR\",\"CXT\",\"CACC\",\"CRH\",\"CROX\",\"CRWD\",\"CCI\",\"CCK\",\"CSX\",\"CUBE\",\"CMI\",\n",
    "               \"CW\",\"CVS\",\"DHI\",\"DHR\",\"DRI\",\"DAR\",\"DDOG\",\"DVA\",\"DAY\",\"DECK\",\"DE\",\"DAL\",\"DELL\",\n",
    "               \"XRAY\",\"DVN\",\"DXCM\",\"FANG\",\"DKS\",\"DLR\",\"DDS\",\"DOCU\",\"DLB\",\"DG\",\"DLTR\",\"D\",\"DPZ\",\n",
    "               \"DCI\",\"DASH\",\"DV\",\"DOV\",\"DOW\",\"DOCS\",\"DKNG\",\"DBX\",\"DTM\",\"DTE\",\"DUK\",\"DUOL\",\"DD\",\n",
    "               \"BROS\",\"DXC\",\"DT\",\"ELF\",\"EXP\",\"EWBC\",\"EGP\",\"EMN\",\"ETN\",\"EBAY\",\"ECL\",\"EIX\",\"EW\",\n",
    "               \"ELAN\",\"ESTC\",\"EA\",\"ESI\",\"ELV\",\"EME\",\"EMR\",\"EHC\",\"ENPH\",\"ENTG\",\"ETR\",\"NVST\",\"EOG\",\n",
    "               \"EPAM\",\"EPR\",\"EQT\",\"EFX\",\"EQIX\",\"EQH\",\"ELS\",\"EQR\",\"ESAB\",\"WTRG\",\"ESS\",\"EL\",\"ETSY\",\n",
    "               \"EEFT\",\"EVR\",\"EG\",\"EVRG\",\"ES\",\"ECG\",\"EXAS\",\"EXEL\",\"EXC\",\"EXLS\",\"EXE\",\"EXPE\",\"EXPD\",\n",
    "               \"EXR\",\"XOM\",\"FFIV\",\"FDS\",\"FICO\",\"FAST\",\"FRT\",\"FDX\",\"FERG\",\"FNF\",\"FIS\",\"FITB\",\"FAF\",\n",
    "               \"FCNCA\",\"FHB\",\"FHN\",\"FR\",\"FSLR\",\"FE\",\"FI\",\"FIVE\",\"FLEX\",\"FND\",\"FLO\",\"FLS\",\"FLUT\",\n",
    "               \"FMC\",\"FNB\",\"F\",\"FTNT\",\"FTV\",\"FBIN\",\"FOXA\",\"FOX\",\"BEN\",\"FRHC\",\"FCX\",\"FRPT\",\"FYBR\",\n",
    "               \"CFR\",\"FTAI\",\"FCN\",\"GME\",\"GLPI\",\"GAP\",\"GRMN\",\"IT\",\"GTES\",\"GLIBA\",\"GLIBK\",\"GE\",\n",
    "               \"GEHC\",\"GEV\",\"GEN\",\"GNRC\",\"GD\",\"GIS\",\"GM\",\"G\",\"GNTX\",\"GPC\",\"GILD\",\"GTLB\",\"GPN\",\n",
    "               \"GFS\",\"GLOB\",\"GL\",\"GMED\",\"GDDY\",\"GS\",\"GGG\",\"LOPE\",\"GPK\",\"GWRE\",\"GXO\",\"HAL\",\"HALO\",\n",
    "               \"HLNE\",\"THG\",\"HOG\",\"HIG\",\"HAS\",\"HAYW\",\"HCA\",\"HR\",\"DOC\",\"HEI-A\",\"HEI\",\"JKHY\",\"HSY\",\n",
    "               \"HPE\",\"HXL\",\"DINO\",\"HIW\",\"HLT\",\"HOLX\",\"HD\",\"HON\",\"HRL\",\"HST\",\"HLI\",\"HHH\",\"HWM\",\n",
    "               \"HPQ\",\"HUBB\",\"HUBS\",\"HUM\",\"HBAN\",\"HII\",\"HUN\",\"H\",\"IAC\",\"IBM\",\"IDA\",\"IEX\",\"IDXX\",\n",
    "               \"ITW\",\"ILMN\",\"INCY\",\"INFA\",\"IR\",\"INGM\",\"INGR\",\"INSM\",\"INSP\",\"PODD\",\"INTC\",\"IBKR\",\n",
    "               \"ICE\",\"IFF\",\"IP\",\"IPG\",\"INTU\",\"ISRG\",\"IVZ\",\"INVH\",\"IONS\",\"IPGP\",\"IQV\",\"IRDM\",\n",
    "               \"IRM\",\"ITT\",\"JBL\",\"J\",\"JHX\",\"JHG\",\"JAZZ\",\"JBHT\",\"JEF\",\"JNJ\",\"JCI\",\"JLL\",\"JPM\",\n",
    "               \"KRMN\",\"KBR\",\"K\",\"KMPR\",\"KVUE\",\"KDP\",\"KEY\",\"KEYS\",\"KRC\",\"KMB\",\"KIM\",\"KMI\",\"KNSL\",\n",
    "               \"KEX\",\"KKR\",\"KLAC\",\"KNX\",\"KHC\",\"KR\",\"KD\",\"LHX\",\"LH\",\"LRCX\",\"LAMR\",\"LW\",\"LSTR\",\n",
    "               \"LVS\",\"LSCC\",\"LAZ\",\"LEA\",\"LDOS\",\"LEN\",\"LEN-B\",\"LII\",\"DRS\",\"LBRDA\",\"LBRDK\",\"LBTYA\",\n",
    "               \"LBTYK\",\"FWONA\",\"FWONK\",\"LLYVA\",\"LLYVK\",\"LNW\",\"LLY\",\"LECO\",\"LNC\",\"LIN\",\"LINE\",\n",
    "               \"LAD\",\"LFUS\",\"LYV\",\"LKQ\",\"LOAR\",\"LMT\",\"L\",\"LPX\",\"LOW\",\"LPLA\",\"LCID\",\"LULU\",\"LITE\",\n",
    "               \"LYFT\",\"LYB\",\"MTB\",\"MTSI\",\"M\",\"MSGS\",\"MANH\",\"MAN\",\"CART\",\"MPC\",\"MKL\",\"MKTX\",\n",
    "               \"MAR\",\"MMC\",\"MLM\",\"MRVL\",\"MAS\",\"MASI\",\"MTZ\",\"MA\",\"MTDR\",\"MTCH\",\"MAT\",\"MKC\",\n",
    "               \"MCD\",\"MCK\",\"MDU\",\"MPW\",\"MEDP\",\"MDT\",\"MRK\",\"META\",\"MET\",\"MTD\",\"MTG\",\"MGM\",\n",
    "               \"MCHP\",\"MU\",\"MSFT\",\"MSTR\",\"MAA\",\"MIDD\",\"TIGO\",\"MRP\",\"MKSI\",\"MRNA\",\"MHK\",\"MOH\",\n",
    "               \"TAP\",\"MDLZ\",\"MDB\",\"MPWR\",\"MNST\",\"MCO\",\"MS\",\"MORN\",\"MOS\",\"MSI\",\"MP\",\"MSA\",\"MSM\",\n",
    "               \"MSCI\",\"MLI\",\"MUSA\",\"NDAQ\",\"NTRA\",\"NFG\",\"NSA\",\"NCNO\",\"NTAP\",\"NFLX\",\"NBIX\",\"NYT\",\n",
    "               \"NWL\",\"NEU\",\"NEM\",\"NWSA\",\"NWS\",\"NXST\",\"NEE\",\"NIQ\",\"NKE\",\"NI\",\"NNN\",\"NDSN\",\"NSC\",\n",
    "               \"NTRS\",\"NOC\",\"NCLH\",\"NOV\",\"NRG\",\"NU\",\"NUE\",\"NTNX\",\"NVT\",\"NVDA\",\"NVR\",\"ORLY\",\"OXY\",\n",
    "               \"OGE\",\"OKTA\",\"ODFL\",\"ORI\",\"OLN\",\"OLLI\",\"OHI\",\"OMC\",\"ONON\",\"ON\",\"OMF\",\"OKE\",\"ONTO\",\n",
    "               \"ORCL\",\"OGN\",\"OSK\",\"OTIS\",\"OVV\",\"OC\",\"PCAR\",\"PKG\",\"PLTR\",\"PANW\",\"PK\",\"PH\",\"PSN\",\n",
    "               \"PAYX\",\"PAYC\",\"PCTY\",\"PYPL\",\"PEGA\",\"PENN\",\"PAG\",\"PNR\",\"PEN\",\"PEP\",\"PFGC\",\"PR\",\n",
    "               \"PRGO\",\"PFE\",\"PCG\",\"PM\",\"PSX\",\"PPC\",\"PNFP\",\"PNW\",\"PINS\",\"PLNT\",\"PNC\",\"POOL\",\n",
    "               \"BPOP\",\"POST\",\"PPG\",\"PPL\",\"TROW\",\"PRI\",\"PRMB\",\"PFG\",\"PCOR\",\"PG\",\"PGR\",\"PLD\",\n",
    "               \"PB\",\"PRU\",\"PTC\",\"PSA\",\"PEG\",\"PHM\",\"PSTG\",\"PVH\",\"QGEN\",\"QRVO\",\"QCOM\",\"PWR\",\"QS\",\n",
    "               \"DGX\",\"QXO\",\"RAL\",\"RL\",\"RRC\",\"RJF\",\"RYN\",\"RBA\",\"RBC\",\"O\",\"RDDT\",\"RRX\",\"REG\",\n",
    "               \"REGN\",\"RF\",\"RGA\",\"RS\",\"RNR\",\"RGEN\",\"RSG\",\"RMD\",\"QSR\",\"RVMD\",\"RVTY\",\"REXR\",\n",
    "               \"REYN\",\"RH\",\"RNG\",\"RITM\",\"RIVN\",\"RLI\",\"RHI\",\"HOOD\",\"RBLX\",\"RKT\",\"RKLB\",\"ROK\",\n",
    "               \"ROIV\",\"ROKU\",\"ROL\",\"ROP\",\"ROST\",\"RCL\",\"RGLD\",\"RPRX\",\"RPM\",\"RTX\",\"RBRK\",\"RYAN\",\n",
    "               \"R\",\"SPGI\",\"SAIA\",\"SAIL\",\"SAIC\",\"CRM\",\"SLM\",\"IOT\",\"SNDK\",\"SRPT\",\"SBAC\",\"HSIC\",\n",
    "               \"SLB\",\"SNDR\",\"SMG\",\"SEB\",\"SEE\",\"SEIC\",\"SRE\",\"ST\",\"S\",\"SCI\",\"NOW\",\"SN\",\"SHW\",\n",
    "               \"FOUR\",\"SLGN\",\"SPG\",\"SSD\",\"SIRI\",\"SITE\",\"SWKS\",\"SFD\",\"SJM\",\"SW\",\"SNA\",\"SNOW\",\n",
    "               \"SOFI\",\"SOLS\",\"SOLV\",\"SGI\",\"SON\",\"SHC\",\"SO\",\"SCCO\",\"SSB\",\"LUV\",\"SPR\",\"SPOT\",\n",
    "               \"SFM\",\"SSNC\",\"STAG\",\"SARO\",\"SWK\",\"SBUX\",\"STWD\",\"STT\",\"STLD\",\"STE\",\"SF\",\"SYK\",\n",
    "               \"SMMT\",\"SUI\",\"SMCI\",\"SYF\",\"SNPS\",\"SNV\",\"SYY\",\"TMUS\",\"TTWO\",\"TLN\",\"TPR\",\"TRGP\",\n",
    "               \"TGT\",\"SNX\",\"FTI\",\"TDY\",\"TFX\",\"TEM\",\"THC\",\"TDC\",\"TER\",\"TSLA\",\"TTEK\",\"TXN\",\"TPL\",\n",
    "               \"TXRH\",\"TXT\",\"TMO\",\"TFSL\",\"THO\",\"TKR\",\"TJX\",\"TKO\",\"TOST\",\"TOL\",\"BLD\",\"TTC\",\n",
    "               \"TPG\",\"TSCO\",\"TTD\",\"TW\",\"TT\",\"TDG\",\"TRU\",\"TNL\",\"TRV\",\"TREX\",\"TRMB\",\"TFC\",\n",
    "               \"DJT\",\"TWLO\",\"TYL\",\"TSN\",\"UHAL\",\"UHAL-B\",\"USB\",\"UBER\",\"UI\",\"UDR\",\"UGI\",\"PATH\",\n",
    "               \"ULTA\",\"RARE\",\"UAA\",\"UA\",\"UNP\",\"UAL\",\"UPS\",\"URI\",\"UTHR\",\"UWMC\",\"UNH\",\"U\",\"OLED\",\n",
    "               \"UHS\",\"UNM\",\"USFD\",\"MTN\",\"VLO\",\"VMI\",\"VVV\",\"VEEV\",\"VTR\",\"VLTO\",\"VRSN\",\"VRSK\",\n",
    "               \"VZ\",\"VRTX\",\"VRT\",\"VFC\",\"VTRS\",\"VICI\",\"VIK\",\"VKTX\",\"VNOM\",\"VIRT\",\"V\",\"VST\",\n",
    "               \"VNT\",\"VNO\",\"VOYA\",\"VMC\",\"WPC\",\"WRB\",\"GWW\",\"WAB\",\"WMT\",\"DIS\",\"WBD\",\"WM\",\"WAT\",\n",
    "               \"WSO\",\"W\",\"WFRD\",\"WBS\",\"WEC\",\"WFC\",\"WELL\",\"WEN\",\"WCC\",\"WST\",\"WAL\",\"WDC\",\"WU\",\n",
    "               \"WLK\",\"WEX\",\"WY\",\"WHR\",\"WTM\",\"WMB\",\"WSM\",\"WTW\",\"WSC\",\"WING\",\"WTFC\",\"WWD\",\"WDAY\",\n",
    "               \"WH\",\"WYNN\",\"XEL\",\"XP\",\"XPO\",\"XYL\",\"YETI\",\"YUM\",\"ZBRA\",\"ZG\",\"Z\",\"ZBH\",\"ZION\",\n",
    "               \"ZTS\",\"ZM\",\"GTM\",\"ZS\"]\n",
    "print(f\"‚úÖ Loaded {len(all_tickers)} tickers (embedded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdc4c4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Core Functions\n",
    "\n",
    "### Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aa38c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Functions defined successfully\n"
     ]
    }
   ],
   "source": [
    "def download_ticker_data(ticker, start_date, end_date, interval=\"30m\"):\n",
    "    \"\"\"\n",
    "    Download stock data for a single ticker\n",
    "    Returns DataFrame with only required columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download data from Yahoo Finance\n",
    "        df = yf.download(\n",
    "            ticker, \n",
    "            start=start_date, \n",
    "            end=end_date, \n",
    "            interval=interval,\n",
    "            progress=False  # Disable yfinance progress bar\n",
    "        )\n",
    "        \n",
    "        if df.empty:\n",
    "            logging.warning(f\"No data available for {ticker}\")\n",
    "            return None\n",
    "        \n",
    "        # Flatten MultiIndex columns if present\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = df.columns.get_level_values(0)\n",
    "        \n",
    "        # Reset index to make Datetime a column\n",
    "        df = df.reset_index()\n",
    "        \n",
    "        # Add ticker column\n",
    "        df['Ticker'] = ticker\n",
    "        \n",
    "        # Select only required columns\n",
    "        required_columns = ['Datetime', 'Close', 'High', 'Low', 'Open', 'Volume', 'Ticker']\n",
    "        df = df[required_columns]\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_data(ticker, df):\n",
    "    \"\"\"\n",
    "    Save data to compressed CSV file (append mode)\n",
    "    \"\"\"\n",
    "    file_path = DATA_DIR / f\"{ticker}.csv.gz\"\n",
    "    \n",
    "    try:\n",
    "        # If file exists, load and append new data\n",
    "        if file_path.exists():\n",
    "            existing_df = pd.read_csv(file_path, compression='gzip', parse_dates=['Datetime'])\n",
    "            df = pd.concat([existing_df, df], ignore_index=True)\n",
    "            # Remove duplicates based on Datetime and Ticker\n",
    "            df = df.drop_duplicates(subset=['Datetime', 'Ticker'], keep='last')\n",
    "        \n",
    "        # Sort by Datetime\n",
    "        df = df.sort_values('Datetime')\n",
    "        \n",
    "        # Save to compressed CSV\n",
    "        df.to_csv(file_path, compression='gzip', index=False)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data for {ticker}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_all(tickers, start_date, end_date, interval=\"30m\", delay=0.2):\n",
    "    \"\"\"\n",
    "    Download data for all tickers with progress bar\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    for ticker in tqdm(tickers, desc=\"üì• Downloading\", ncols=100, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):\n",
    "        df = download_ticker_data(ticker, start_date, end_date, interval)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            if save_data(ticker, df):\n",
    "                successful += 1\n",
    "                logging.info(f\"‚úÖ {ticker}: {len(df)} records saved\")\n",
    "            else:\n",
    "                failed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            logging.error(f\"‚ùå {ticker}: Failed to download\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "print(\"‚úÖ Functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae7ec4e",
   "metadata": {},
   "source": [
    "### Save Function (CSV with compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58c18ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Save function defined\n"
     ]
    }
   ],
   "source": [
    "def save_data(df, ticker, append=True):\n",
    "    \"\"\"\n",
    "    Save data to compressed CSV file\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Data to save\n",
    "        ticker (str): Ticker symbol\n",
    "        append (bool): If True, append to existing file\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = DATA_DIR / f\"{ticker}.csv.gz\"\n",
    "        \n",
    "        if append and file_path.exists():\n",
    "            # Read existing data\n",
    "            existing_df = pd.read_csv(file_path, compression='gzip', parse_dates=['Datetime'])\n",
    "            \n",
    "            # Merge with new data\n",
    "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            combined_df = combined_df.drop_duplicates(subset=['Datetime', 'Ticker'], keep='last')\n",
    "            \n",
    "            # Sort by datetime\n",
    "            combined_df = combined_df.sort_values('Datetime').reset_index(drop=True)\n",
    "            \n",
    "            combined_df.to_csv(file_path, compression='gzip', index=False)\n",
    "            logging.info(f\"{ticker}: Added {len(df)} new records (total: {len(combined_df)})\")\n",
    "        else:\n",
    "            df.to_csv(file_path, compression='gzip', index=False)\n",
    "            logging.info(f\"{ticker}: Saved {len(df)} records\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"{ticker}: Save error - {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Save function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf47b5d",
   "metadata": {},
   "source": [
    "### Batch Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c770cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch download function defined\n"
     ]
    }
   ],
   "source": [
    "def download_all(tickers, start_date, end_date, interval=\"30m\", delay=0.2):\n",
    "    \"\"\"\n",
    "    Download data for all tickers with error handling\n",
    "    \n",
    "    Args:\n",
    "        tickers (list): List of ticker symbols\n",
    "        start_date (str): Start date\n",
    "        end_date (str): End date\n",
    "        interval (str): Data interval\n",
    "        delay (float): Delay between requests (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (successful_count, failed_count)\n",
    "    \"\"\"\n",
    "    total = len(tickers)\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    logging.info(f\"Starting download of {total} tickers...\")\n",
    "    logging.info(f\"Period: {start_date} to {end_date}\")\n",
    "    logging.info(f\"Interval: {interval}\")\n",
    "    \n",
    "    for i, ticker in enumerate(tickers, 1):\n",
    "        logging.info(f\"[{i}/{total}] Processing {ticker}...\")\n",
    "        \n",
    "        # Download data\n",
    "        df = download_ticker_data(ticker, start_date, end_date, interval)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # Save data\n",
    "            if save_data(df, ticker, append=True):\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "        \n",
    "        # Delay to avoid rate limiting\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    # Summary\n",
    "    logging.info(\"=\" * 50)\n",
    "    logging.info(\"Download complete!\")\n",
    "    logging.info(f\"Successful: {successful}/{total}\")\n",
    "    logging.info(f\"Failed: {failed}/{total}\")\n",
    "    logging.info(\"=\" * 50)\n",
    "    \n",
    "    return successful, failed\n",
    "\n",
    "print(\"‚úÖ Batch download function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f42a9",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Calculate Date Range\n",
    "\n",
    "Automatically set to download data from **2 days ago to today** (3 days total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3861f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Download period: 2025-11-03 to 2025-11-05\n",
      "üìÜ Days included: 3 days (from 2 days ago to today)\n",
      "‚è∞ Interval: 30 minutes\n"
     ]
    }
   ],
   "source": [
    "# Calculate dates (from 2 days ago to today)\n",
    "# Example: if today is Nov 5, downloads data from Nov 3, 4, and 5\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=2)\n",
    "\n",
    "# Format \n",
    "start_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"üìÖ Download period: {start_str} to {end_str}\")\n",
    "print(f\"üìÜ Days included: 3 days (from 2 days ago to today)\")\n",
    "print(f\"‚è∞ Interval: 30 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5763c7",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Run Download\n",
    "\n",
    "### Option A: Test with Sample Tickers (Recommended First)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e0da87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Test with a small sample\\ntest_tickers = [\\'AAPL\\', \\'MSFT\\', \\'GOOGL\\', \\'TSLA\\', \\'NVDA\\', \\'BTC-USD\\', \\'ETH-USD\\', \\'BNB-USD\\']\\n\\nprint(f\"üß™ TEST MODE - Downloading {len(test_tickers)} tickers...\\n\")\\n\\nsuccessful, failed = download_all(\\n    tickers=test_tickers,\\n    start_date=start_str,\\n    end_date=end_str,\\n    interval=\"30m\",\\n    delay=0.2\\n)\\n\\nprint(f\"\\n‚úÖ Test complete: {successful} successful, {failed} failed\") '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Test with a small sample\n",
    "test_tickers = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'BTC-USD', 'ETH-USD', 'BNB-USD']\n",
    "\n",
    "print(f\"üß™ TEST MODE - Downloading {len(test_tickers)} tickers...\\n\")\n",
    "\n",
    "successful, failed = download_all(\n",
    "    tickers=test_tickers,\n",
    "    start_date=start_str,\n",
    "    end_date=end_str,\n",
    "    interval=\"30m\",\n",
    "    delay=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Test complete: {successful} successful, {failed} failed\") \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8787f86",
   "metadata": {},
   "source": [
    "### Option B: Full Download (All Tickers)\n",
    "\n",
    "‚ö†Ô∏è **This will take approximately 6-7 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f3f49f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Starting download of 1007 tickers...\n",
      "‚è±Ô∏è  Estimated time: ~5.9 minutes\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Total time: 6.78 minutes\n",
      "‚úÖ Successfully downloaded: 1007/1007 tickers\n",
      "‚ùå Failed: 0/1007 tickers\n"
     ]
    }
   ],
   "source": [
    "# Full download of all tickers\n",
    "print(f\"üì• Starting download of {len(all_tickers)} tickers...\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: ~{len(all_tickers) * 0.35 / 60:.1f} minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "successful, failed = download_all(\n",
    "    tickers=all_tickers,\n",
    "    start_date=start_str,\n",
    "    end_date=end_str,\n",
    "    interval=\"30m\",\n",
    "    delay=0.2\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total time: {elapsed_time/60:.2f} minutes\")\n",
    "print(f\"‚úÖ Successfully downloaded: {successful}/{len(all_tickers)} tickers\")\n",
    "print(f\"‚ùå Failed: {failed}/{len(all_tickers)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df582ef",
   "metadata": {},
   "source": [
    "If some errors occured, no problem: check the download_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a028e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ FUNZIONI PER RE-DOWNLOAD TICKERS SPECIFICI\n",
    "\n",
    "def redownload_tickers(tickers, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Re-download specifici tickers\n",
    "    Esempi:\n",
    "        redownload_tickers('AAPL')\n",
    "        redownload_tickers(['AAPL', 'MSFT', 'GOOGL'])\n",
    "    \"\"\"\n",
    "    if isinstance(tickers, str):\n",
    "        tickers = [tickers]\n",
    "    \n",
    "    if start_date is None:\n",
    "        start_date = start_str\n",
    "    if end_date is None:\n",
    "        end_date = end_str\n",
    "    \n",
    "    print(f\"üîÑ Re-downloading {len(tickers)} ticker(s)...\")\n",
    "    return download_all(tickers, start_date, end_date, \"30m\", 0.2)\n",
    "\n",
    "\n",
    "def get_missing_tickers():\n",
    "    \"\"\"Trova ticker senza file\"\"\"\n",
    "    existing = {f.stem for f in DATA_DIR.glob(\"*.csv.gz\")}\n",
    "    return [t for t in all_tickers if t not in existing]\n",
    "\n",
    "\n",
    "# ESEMPIO USO:\n",
    "# redownload_tickers('MDT')\n",
    "# redownload_tickers(['AAPL', 'MSFT', 'GOOGL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2bfeb",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Verify Downloaded Data\n",
    "\n",
    "### Check Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c91375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Total files saved: 1007\n",
      "\n",
      "üìä Sample Statistics:\n",
      "\n",
      "  AAPL:\n",
      "    ‚Ä¢ Records: 26\n",
      "    ‚Ä¢ Period: 2025-11-03 14:30:00+00:00 to 2025-11-04 20:30:00+00:00\n",
      "    ‚Ä¢ File size: 0.87 KB\n",
      "\n",
      "  MSFT:\n",
      "    ‚Ä¢ Records: 26\n",
      "    ‚Ä¢ Period: 2025-11-03 14:30:00+00:00 to 2025-11-04 20:30:00+00:00\n",
      "    ‚Ä¢ File size: 0.87 KB\n",
      "\n",
      "  GOOGL:\n",
      "    ‚Ä¢ Records: 26\n",
      "    ‚Ä¢ Period: 2025-11-03 14:30:00+00:00 to 2025-11-04 20:30:00+00:00\n",
      "    ‚Ä¢ File size: 0.84 KB\n",
      "\n",
      "  MTD:\n",
      "    ‚Ä¢ Records: 26\n",
      "    ‚Ä¢ Period: 2025-11-03 14:30:00+00:00 to 2025-11-04 20:30:00+00:00\n",
      "    ‚Ä¢ File size: 0.80 KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count saved files\n",
    "csv_files = list(DATA_DIR.glob(\"*.csv.gz\"))\n",
    "print(f\"üìÅ Total files saved: {len(csv_files)}\")\n",
    "\n",
    "# Show statistics for sample tickers\n",
    "sample_tickers = ['AAPL', 'MSFT', 'GOOGL', 'MTD']\n",
    "\n",
    "print(\"\\nüìä Sample Statistics:\\n\")\n",
    "for ticker in sample_tickers:\n",
    "    file_path = DATA_DIR / f\"{ticker}.csv.gz\"\n",
    "    if file_path.exists():\n",
    "        df = pd.read_csv(file_path, compression='gzip', parse_dates=['Datetime'])\n",
    "        print(f\"  {ticker}:\")\n",
    "        print(f\"    ‚Ä¢ Records: {len(df)}\")\n",
    "        print(f\"    ‚Ä¢ Period: {df['Datetime'].min()} to {df['Datetime'].max()}\")\n",
    "        print(f\"    ‚Ä¢ File size: {file_path.stat().st_size / 1024:.2f} KB\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"  {ticker}: ‚ùå File not found\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f37fe9",
   "metadata": {},
   "source": [
    "### View Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41b0b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Sample data from BRK-B:\n",
      "\n",
      "                   Datetime       Close        High         Low        Open  \\\n",
      "0 2025-11-03 14:30:00+00:00  473.950012  479.989990  473.149994  479.510010   \n",
      "1 2025-11-03 15:00:00+00:00  477.079987  477.489014  473.609985  474.019989   \n",
      "2 2025-11-03 15:30:00+00:00  477.862305  478.878387  476.000000  477.140015   \n",
      "3 2025-11-03 16:00:00+00:00  475.010010  477.855011  474.820007  477.850006   \n",
      "4 2025-11-03 16:30:00+00:00  475.690002  476.714508  474.700012  475.019989   \n",
      "5 2025-11-03 17:00:00+00:00  476.605011  477.000000  475.119995  475.690002   \n",
      "6 2025-11-03 17:30:00+00:00  477.320007  477.769989  476.630005  476.690002   \n",
      "7 2025-11-03 18:00:00+00:00  477.109985  477.440002  476.750000  477.290009   \n",
      "8 2025-11-03 18:30:00+00:00  477.225006  478.029999  477.000000  477.165009   \n",
      "9 2025-11-03 19:00:00+00:00  477.100006  477.359985  476.200012  477.269989   \n",
      "\n",
      "    Volume Ticker  \n",
      "0  1107179  BRK-B  \n",
      "1   505015  BRK-B  \n",
      "2   404784  BRK-B  \n",
      "3   308967  BRK-B  \n",
      "4   223717  BRK-B  \n",
      "5   164397  BRK-B  \n",
      "6   170889  BRK-B  \n",
      "7   174955  BRK-B  \n",
      "8   199360  BRK-B  \n",
      "9   216960  BRK-B  \n",
      "\n",
      "üìä Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26 entries, 0 to 25\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype              \n",
      "---  ------    --------------  -----              \n",
      " 0   Datetime  26 non-null     datetime64[ns, UTC]\n",
      " 1   Close     26 non-null     float64            \n",
      " 2   High      26 non-null     float64            \n",
      " 3   Low       26 non-null     float64            \n",
      " 4   Open      26 non-null     float64            \n",
      " 5   Volume    26 non-null     int64              \n",
      " 6   Ticker    26 non-null     object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(4), int64(1), object(1)\n",
      "memory usage: 1.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load and display sample data\n",
    "ticker_example = 'BRK-B'\n",
    "file_path = DATA_DIR / f\"{ticker_example}.csv.gz\"\n",
    "\n",
    "if file_path.exists():\n",
    "    df = pd.read_csv(file_path, compression='gzip', parse_dates=['Datetime'])\n",
    "    \n",
    "    print(f\"üìà Sample data from {ticker_example}:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    print(f\"\\nüìä Dataset info:\")\n",
    "    print(df.info())\n",
    "else:\n",
    "    print(f\"‚ùå File {ticker_example}.csv.gz not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcc556",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Load Specific Ticker Data\n",
    "\n",
    "Helper function to easily load any ticker's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6107655c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-03 14:30:00+00:00</td>\n",
       "      <td>247.729996</td>\n",
       "      <td>257.064209</td>\n",
       "      <td>247.729996</td>\n",
       "      <td>255.899994</td>\n",
       "      <td>316608</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-03 15:00:00+00:00</td>\n",
       "      <td>244.759995</td>\n",
       "      <td>248.039993</td>\n",
       "      <td>244.190002</td>\n",
       "      <td>247.925003</td>\n",
       "      <td>338951</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-03 15:30:00+00:00</td>\n",
       "      <td>245.580002</td>\n",
       "      <td>246.800003</td>\n",
       "      <td>243.820007</td>\n",
       "      <td>244.875107</td>\n",
       "      <td>174541</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-03 16:00:00+00:00</td>\n",
       "      <td>244.574997</td>\n",
       "      <td>245.809998</td>\n",
       "      <td>244.460007</td>\n",
       "      <td>245.580002</td>\n",
       "      <td>158834</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-03 16:30:00+00:00</td>\n",
       "      <td>241.919998</td>\n",
       "      <td>245.110001</td>\n",
       "      <td>241.919998</td>\n",
       "      <td>244.574997</td>\n",
       "      <td>251911</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-03 17:00:00+00:00</td>\n",
       "      <td>243.475006</td>\n",
       "      <td>245.149994</td>\n",
       "      <td>241.089996</td>\n",
       "      <td>241.669998</td>\n",
       "      <td>344838</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-03 17:30:00+00:00</td>\n",
       "      <td>244.735001</td>\n",
       "      <td>245.020004</td>\n",
       "      <td>242.889999</td>\n",
       "      <td>243.365005</td>\n",
       "      <td>188333</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-03 18:00:00+00:00</td>\n",
       "      <td>243.570007</td>\n",
       "      <td>245.080002</td>\n",
       "      <td>243.050003</td>\n",
       "      <td>244.649994</td>\n",
       "      <td>166823</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-03 18:30:00+00:00</td>\n",
       "      <td>243.020004</td>\n",
       "      <td>244.145004</td>\n",
       "      <td>242.860001</td>\n",
       "      <td>243.524994</td>\n",
       "      <td>167577</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-03 19:00:00+00:00</td>\n",
       "      <td>242.240005</td>\n",
       "      <td>243.226807</td>\n",
       "      <td>242.029999</td>\n",
       "      <td>243.110001</td>\n",
       "      <td>982791</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-03 19:30:00+00:00</td>\n",
       "      <td>242.990005</td>\n",
       "      <td>243.750000</td>\n",
       "      <td>242.130096</td>\n",
       "      <td>242.270004</td>\n",
       "      <td>165890</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-11-03 20:00:00+00:00</td>\n",
       "      <td>241.940002</td>\n",
       "      <td>242.990005</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>242.934998</td>\n",
       "      <td>311260</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-11-03 20:30:00+00:00</td>\n",
       "      <td>241.940002</td>\n",
       "      <td>242.100006</td>\n",
       "      <td>241.020996</td>\n",
       "      <td>241.964996</td>\n",
       "      <td>603264</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-11-04 14:30:00+00:00</td>\n",
       "      <td>243.110001</td>\n",
       "      <td>243.250000</td>\n",
       "      <td>236.389999</td>\n",
       "      <td>238.410004</td>\n",
       "      <td>487220</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-11-04 15:00:00+00:00</td>\n",
       "      <td>243.964996</td>\n",
       "      <td>244.169998</td>\n",
       "      <td>242.089996</td>\n",
       "      <td>243.270004</td>\n",
       "      <td>234238</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-11-04 15:30:00+00:00</td>\n",
       "      <td>242.020004</td>\n",
       "      <td>244.059998</td>\n",
       "      <td>241.910004</td>\n",
       "      <td>243.960007</td>\n",
       "      <td>188701</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-11-04 16:00:00+00:00</td>\n",
       "      <td>242.389999</td>\n",
       "      <td>242.559998</td>\n",
       "      <td>241.119995</td>\n",
       "      <td>241.929993</td>\n",
       "      <td>220499</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-11-04 16:30:00+00:00</td>\n",
       "      <td>241.672501</td>\n",
       "      <td>242.309998</td>\n",
       "      <td>240.860001</td>\n",
       "      <td>242.279999</td>\n",
       "      <td>135701</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-11-04 17:00:00+00:00</td>\n",
       "      <td>242.360001</td>\n",
       "      <td>242.679993</td>\n",
       "      <td>241.039993</td>\n",
       "      <td>241.684998</td>\n",
       "      <td>730603</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-11-04 17:30:00+00:00</td>\n",
       "      <td>242.759995</td>\n",
       "      <td>242.919998</td>\n",
       "      <td>241.470001</td>\n",
       "      <td>242.399994</td>\n",
       "      <td>143633</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-11-04 18:00:00+00:00</td>\n",
       "      <td>243.270004</td>\n",
       "      <td>243.919998</td>\n",
       "      <td>241.929993</td>\n",
       "      <td>242.649994</td>\n",
       "      <td>217995</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-11-04 18:30:00+00:00</td>\n",
       "      <td>242.199997</td>\n",
       "      <td>243.740005</td>\n",
       "      <td>241.800003</td>\n",
       "      <td>243.229996</td>\n",
       "      <td>873885</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-11-04 19:00:00+00:00</td>\n",
       "      <td>241.315002</td>\n",
       "      <td>242.300003</td>\n",
       "      <td>240.589996</td>\n",
       "      <td>242.110001</td>\n",
       "      <td>186047</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-11-04 19:30:00+00:00</td>\n",
       "      <td>241.300003</td>\n",
       "      <td>241.500000</td>\n",
       "      <td>240.289993</td>\n",
       "      <td>241.419907</td>\n",
       "      <td>142407</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-11-04 20:00:00+00:00</td>\n",
       "      <td>240.824997</td>\n",
       "      <td>241.660004</td>\n",
       "      <td>240.339996</td>\n",
       "      <td>241.350006</td>\n",
       "      <td>218234</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-11-04 20:30:00+00:00</td>\n",
       "      <td>239.960007</td>\n",
       "      <td>241.289993</td>\n",
       "      <td>239.229996</td>\n",
       "      <td>240.824997</td>\n",
       "      <td>492549</td>\n",
       "      <td>DASH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime       Close        High         Low        Open  \\\n",
       "0  2025-11-03 14:30:00+00:00  247.729996  257.064209  247.729996  255.899994   \n",
       "1  2025-11-03 15:00:00+00:00  244.759995  248.039993  244.190002  247.925003   \n",
       "2  2025-11-03 15:30:00+00:00  245.580002  246.800003  243.820007  244.875107   \n",
       "3  2025-11-03 16:00:00+00:00  244.574997  245.809998  244.460007  245.580002   \n",
       "4  2025-11-03 16:30:00+00:00  241.919998  245.110001  241.919998  244.574997   \n",
       "5  2025-11-03 17:00:00+00:00  243.475006  245.149994  241.089996  241.669998   \n",
       "6  2025-11-03 17:30:00+00:00  244.735001  245.020004  242.889999  243.365005   \n",
       "7  2025-11-03 18:00:00+00:00  243.570007  245.080002  243.050003  244.649994   \n",
       "8  2025-11-03 18:30:00+00:00  243.020004  244.145004  242.860001  243.524994   \n",
       "9  2025-11-03 19:00:00+00:00  242.240005  243.226807  242.029999  243.110001   \n",
       "10 2025-11-03 19:30:00+00:00  242.990005  243.750000  242.130096  242.270004   \n",
       "11 2025-11-03 20:00:00+00:00  241.940002  242.990005  241.000000  242.934998   \n",
       "12 2025-11-03 20:30:00+00:00  241.940002  242.100006  241.020996  241.964996   \n",
       "13 2025-11-04 14:30:00+00:00  243.110001  243.250000  236.389999  238.410004   \n",
       "14 2025-11-04 15:00:00+00:00  243.964996  244.169998  242.089996  243.270004   \n",
       "15 2025-11-04 15:30:00+00:00  242.020004  244.059998  241.910004  243.960007   \n",
       "16 2025-11-04 16:00:00+00:00  242.389999  242.559998  241.119995  241.929993   \n",
       "17 2025-11-04 16:30:00+00:00  241.672501  242.309998  240.860001  242.279999   \n",
       "18 2025-11-04 17:00:00+00:00  242.360001  242.679993  241.039993  241.684998   \n",
       "19 2025-11-04 17:30:00+00:00  242.759995  242.919998  241.470001  242.399994   \n",
       "20 2025-11-04 18:00:00+00:00  243.270004  243.919998  241.929993  242.649994   \n",
       "21 2025-11-04 18:30:00+00:00  242.199997  243.740005  241.800003  243.229996   \n",
       "22 2025-11-04 19:00:00+00:00  241.315002  242.300003  240.589996  242.110001   \n",
       "23 2025-11-04 19:30:00+00:00  241.300003  241.500000  240.289993  241.419907   \n",
       "24 2025-11-04 20:00:00+00:00  240.824997  241.660004  240.339996  241.350006   \n",
       "25 2025-11-04 20:30:00+00:00  239.960007  241.289993  239.229996  240.824997   \n",
       "\n",
       "    Volume Ticker  \n",
       "0   316608   DASH  \n",
       "1   338951   DASH  \n",
       "2   174541   DASH  \n",
       "3   158834   DASH  \n",
       "4   251911   DASH  \n",
       "5   344838   DASH  \n",
       "6   188333   DASH  \n",
       "7   166823   DASH  \n",
       "8   167577   DASH  \n",
       "9   982791   DASH  \n",
       "10  165890   DASH  \n",
       "11  311260   DASH  \n",
       "12  603264   DASH  \n",
       "13  487220   DASH  \n",
       "14  234238   DASH  \n",
       "15  188701   DASH  \n",
       "16  220499   DASH  \n",
       "17  135701   DASH  \n",
       "18  730603   DASH  \n",
       "19  143633   DASH  \n",
       "20  217995   DASH  \n",
       "21  873885   DASH  \n",
       "22  186047   DASH  \n",
       "23  142407   DASH  \n",
       "24  218234   DASH  \n",
       "25  492549   DASH  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_ticker(ticker):\n",
    "    \"\"\"\n",
    "    Load data for a specific ticker\n",
    "    \n",
    "    Args:\n",
    "        ticker (str): Ticker symbol\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Ticker data or None if not found\n",
    "    \"\"\"\n",
    "    file_path = DATA_DIR / f\"{ticker}.csv.gz\"\n",
    "    \n",
    "    if file_path.exists():\n",
    "        return pd.read_csv(file_path, compression='gzip', parse_dates=['Datetime'])\n",
    "    else:\n",
    "        print(f\"‚ùå File {ticker}.csv.gz not found\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "df_apple = load_ticker('DASH')\n",
    "df_apple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15e650",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Daily Usage Instructions\n",
    "\n",
    "### To download data every day:\n",
    "\n",
    "1. **Open this notebook**\n",
    "2. **Run all cells** from Section 1-5 (setup and configuration)\n",
    "3. **Run Section 6 - Option B** (Full Download)\n",
    "4. **Check Section 7** to verify the data\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- ‚úÖ **Automatic date calculation** (3 days: from 2 days ago to today)\n",
    "- ‚úÖ **Incremental append** (new data added without overwriting)\n",
    "- ‚úÖ **Duplicate removal** (automatic)\n",
    "- ‚úÖ **Error logging** (check `download_log.txt`)\n",
    "- ‚úÖ **Compressed storage** (CSV.gz format)\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- üìÖ 30-minute data is available for the **last 60 days** only\n",
    "- üìä Stocks have ~14 records/day (market hours only)\n",
    "- üìÅ Tickers are loaded from **tickers.csv** file\n",
    "- üïí Download time depends on the number of tickers in the CSV file\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
